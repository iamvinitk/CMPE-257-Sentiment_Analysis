# -*- coding: utf-8 -*-
"""257_project_filtereddata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vRDyU3qV0dgoucL2KCKwlJVgTo7WgJRL
"""

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords

import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC, SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score

df = pd.read_csv('/content/training1600000.csv',encoding = "ISO-8859-1",header=None)

df.columns = ["polarity",'id','date','query','user','tweet']
df

"""# Missing Values"""

df.isnull().sum()

"""## Text Cleaning"""

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'http\S+',' ',text)
    text = re.sub('[^a-zA-Z]',' ',text)
    text = word_tokenize(text)
    text = [item for item in text if item not in stop_words]
    text = [lemma.lemmatize(w) for w in text]
    text = [i for i in text if len(i)>2]
    text = ' '.join(text)
    return text

stop_words = set(stopwords.words('english'))
lemma = WordNetLemmatizer()

# df_new = df[0:100000]
df_new = df.sample(n=100000)

df_new['polarity'].value_counts()

df_new['clean_tweet'] = df_new['tweet'].apply(clean_text)

x_train = df_new['clean_tweet']

tfidf_vectorizer = TfidfVectorizer(min_df=10)

x_train_tfidf = pd.DataFrame(tfidf_vectorizer.fit_transform(x_train).toarray(), columns=tfidf_vectorizer.get_feature_names_out())

display(x_train_tfidf.shape)
display(x_train_tfidf.head())

model = LinearSVC()

y_train = df_new.polarity

# y_train

df_test = pd.read_csv('/content/testdata.manual.2009.06.14.csv',header=None)

df_test.columns = ["polarity",'id','date','query','user','tweet']
df_test

df_test['clean_tweet'] = df_test['tweet'].apply(clean_text)

x_test = df_test['clean_tweet']
x_test_tfidf = pd.DataFrame(tfidf_vectorizer.transform(x_test).toarray(), columns=tfidf_vectorizer.get_feature_names_out())

y_test = df_test['polarity']

model.fit(x_train_tfidf, y_train)
results = {
    'accuracy': [],
    'f1': []
}
y_pred = model.predict(x_test_tfidf)
results['accuracy'].append(accuracy_score(y_test, y_pred))
results['f1'].append(f1_score(y_test, y_pred, average=None))

results

df_filtered_test = df_test[df_test['polarity'] != 2]

x_test_filtered = df_filtered_test['clean_tweet']
x_test_filtered_tfidf = pd.DataFrame(tfidf_vectorizer.transform(x_test_filtered).toarray(), columns=tfidf_vectorizer.get_feature_names_out())

y_filtered_test = df_filtered_test['polarity']

len(y_filtered_test)

model.fit(x_train_tfidf, y_train)
results_filtered_SVC = {
    'accuracy': [],
    'f1': []
}
y_pred = model.predict(x_test_filtered_tfidf)
results_filtered_SVC['accuracy'].append(accuracy_score(y_filtered_test, y_pred))
results_filtered_SVC['f1'].append(f1_score(y_filtered_test, y_pred, average=None))



results_filtered_SVC

results

from sklearn.linear_model import SGDClassifier

model2 = SGDClassifier(max_iter=10000, tol=1e-3)

model2.fit(x_train_tfidf, y_train)
results_SGD = {
    'accuracy': [],
    'f1': []
}
y_pred = model.predict(x_test_tfidf)
results_SGD['accuracy'].append(accuracy_score(y_test, y_pred))
results_SGD['f1'].append(f1_score(y_test, y_pred, average=None))

results_SGD

model2.fit(x_train_tfidf, y_train)
results_filtered_SGD = {
    'accuracy': [],
    'f1': []
}

y_pred = model.predict(x_test_filtered_tfidf)
results_filtered_SGD['accuracy'].append(accuracy_score(y_filtered_test, y_pred))
results_filtered_SGD['f1'].append(f1_score(y_filtered_test, y_pred, average=None))

results_filtered_SGD