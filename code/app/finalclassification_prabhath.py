# -*- coding: utf-8 -*-
"""FinalClassification_Prabhath.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rNQF02lVrmCtQCoTtFnAVkA82U2UNGEI
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torchvision
import torch
import torchvision

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 

import re
import nltk
nltk.download('stopwords')
import time
import warnings
warnings.filterwarnings("ignore")
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import LancasterStemmer
from sklearn.utils import shuffle


from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

train_on_gpu = torch.cuda.is_available()
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ... To use GPU, go under edit > notebook settings')
else:
    print('CUDA is available!  Training on GPU ...')
    print(gpu_info)

train_df = pd.read_csv('/content/drive/MyDrive/257_Project/train.csv', encoding="ISO-8859-1", header=None)
train_df.columns = ['polarity', 'id', 'date', 'query', 'user', 'tweet']

test_df = pd.read_csv('/content/drive/MyDrive/257_Project/test.csv', encoding="ISO-8859-1", header=None)
test_df.columns = ['polarity', 'id', 'date', 'query', 'user', 'tweet']

train_df.shape

test_df.shape

word_bank = []

# Function to remove predefined stopwords to reduce disk usage
def preprocess(text):
    review = re.sub('[^a-zA-Z]',' ',text) 
    review = review.lower()
    review = review.split()
    ps = LancasterStemmer()
    all_stopwords = stopwords.words('english')
    all_stopwords.remove('not')
    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
    return ' '.join(review)

"""Training the model on 50K samples for now, to check if the model runs properly and how the classfication works.

"""

train_df = shuffle(train_df,random_state=2)
train_df = train_df[1:800000]

train_df['polarity'].value_counts()

X_train = train_df['tweet'].apply(lambda x: preprocess(x))

y_train = train_df['polarity']
le = LabelEncoder()
y = le.fit_transform(y_train)

X_test = test_df['tweet']
y_test = test_df['polarity']

tfidf = TfidfVectorizer(max_features = 100)
X_train_tf = tfidf.fit_transform(X_train).toarray() 
X_test = tfidf.transform(X_test).toarray()

X_train_tf.shape, X_test.shape, y_train.shape, y_test.shape

"""**Logistic Regression**"""

lr = LogisticRegression(random_state = 0)
start_time = time.time()
lr.fit(X_train_tf, y_train) 
print("Execution Time:", time.time()-start_time,"secs")

y_pred_lr = lr.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_lr))

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_lr)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negative','Neutral','Positive'])
cm_display.plot()

"""Current accuracy of the model using logistic regression on 400k samples : ~42%

**Decision Tree**
"""

dc = DecisionTreeClassifier(criterion = 'entropy', random_state = 22)
start_time = time.time()
dc.fit(X_train_tf, y_train)
print("Execution Time:", time.time()-start_time,"secs")

y_pred_dc = dc.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_dc))

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_dc)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negative','Neutral' ,'Positive'])
cm_display.plot()

"""Current accuracy of the model using decision tree classifier on 50k samples : ~41%

**Naive Bayes Classifier**
"""

nb = MultinomialNB()
start_time = time.time()
nb.fit(X_train_tf,y_train)
print("Execution Time:", time.time()-start_time,"secs")

y_pred_nb = nb.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_nb))

"""Current accuracy of the model using decision tree classifier on 400k samples : ~42%"""

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_nb)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negative','Neutral', 'Positive'])
cm_display.plot()

"""Since the model accuracy was very low. I filtered out the neutral tweet label and made it positive. Neutral label tag was removed altogether."""

train_df['polarity'] = train_df['polarity'].replace(4,1)
train_df

test_df

test_df['polarity'] = test_df['polarity'].replace(2,1)
test_df

test_df['polarity'] = test_df['polarity'].replace(4,1)
test_df

X_train = train_df['tweet'].apply(lambda x: preprocess(x))

y_train = train_df['polarity']
le = LabelEncoder()
y = le.fit_transform(y_train)

X_test = test_df['tweet']
y_test = test_df['polarity']

X_train_tf.shape, X_test.shape, y_train.shape, y_test.shape

tfidf = TfidfVectorizer(max_features = 100)
X_train_tf = tfidf.fit_transform(X_train).toarray() 
X_test = tfidf.transform(X_test).toarray()

"""**Logistic Regreession**"""

lrr = LogisticRegression(random_state = 0)
start_time = time.time()
lrr.fit(X_train_tf, y_train) 
print("Execution Time:", time.time()-start_time,"secs")

y_pred_lrr = lrr.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_lrr))

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_lr)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negative','Positive'])
cm_display.plot()

"""Current accuracy of the model using logistic regression on 400k samples : ~74%

**Decision Tree Classifier**
"""

dc = DecisionTreeClassifier(criterion = 'entropy', random_state = 22)
start_time = time.time()
dc.fit(X_train_tf, y_train)
print("Execution Time:", time.time()-start_time,"secs")

y_pred_dc = dc.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_dc))

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_dc)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negaitve','Positive'])
cm_display.plot()

"""Current accuracy of the model using decision tree classifier on 400k samples : ~65%

**Naive Bayes Classifier**
"""

nb = MultinomialNB()
start_time = time.time()
nb.fit(X_train_tf,y_train)
print("Execution Time:", time.time()-start_time,"secs")

y_pred_nb = nb.predict(X_test)
print("Accuracy:\n", accuracy_score(y_test, y_pred_nb))

"""Current accuracy of the model using decision tree classifier on 400k samples : ~72%"""

from sklearn import metrics
confusion_matrix = metrics.confusion_matrix(y_test, y_pred_nb)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['Negative','Positive'])
cm_display.plot()