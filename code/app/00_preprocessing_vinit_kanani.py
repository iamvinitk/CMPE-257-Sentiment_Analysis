# -*- coding: utf-8 -*-
"""[Vinit Kanani] Sentiment Analysis - Data Cleaning & Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uIwfmbn9Um_gDyGTKv4JyTQ_JXViwxP8
"""

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
import swifter

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/training.csv', encoding="ISO-8859-1", header=None)
data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']

print(data.head())

print(f"Size of the dataset: {data.shape}\n\n")
print(f"Missing Values: {data.isnull().sum()}\n\n")
print(f"Duplicated Values: {data.duplicated().sum()} \n")

"""### There are no missing or duplicate values."""

# label 4 to positive and 0 to negative
data = data[['sentiment', 'tweet']]
data['sentiment'] = data['sentiment'].apply(lambda x: "Positive" if x == 4 else "Negative")
print(data.head(3))

# Plot distribution of sentiments
ax = data['sentiment'].value_counts().plot(kind='bar', title='Distribution of data', legend=False)
ax.set_xticklabels(['Negative', 'Positive'], rotation=0)

stop_words = set(stopwords.words('english'))
stop_words.add('quot')
stop_words.add('amp')
lemma = WordNetLemmatizer()


def preprocess(text):
    text = str(text).lower().strip()
    text = re.sub(r'http\S+|www.\S+', '', text)
    text = re.sub(r'@\S+', '', text)
    text = re.sub(r'#\S+', '', text)
    text = re.sub(r'RT', '', text)
    text = re.sub(r'(.)\1\1+', r'\1\1', text)
    text = re.sub("[^a-zA-Z0-9]", " ", text)
    text = word_tokenize(text)
    text = [item for item in text if item not in stop_words]
    text = [lemma.lemmatize(w) for w in text]
    text = [i for i in text if len(i) > 1]
    text = ' '.join(text)
    return text


print(preprocess(
    "https://www.google.com heyyyyyyyy this is an awesome tweet about machine \
     learning @iamvinitk #machine-learning :-) "))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data['tweet'] = data['tweet'].swifter.apply(lambda x : preprocess(x))

print(data.head(3))
data.to_csv("/content/drive/MyDrive/Colab Notebooks/data/training_cleaned.csv")

positive_tweets = data[data["sentiment"] == "Positive"]
print(positive_tweets.shape)
plt.figure(figsize=(20, 20))
wc = WordCloud(max_words=200, width=1600, height=800).generate(" ".join(positive_tweets["tweet"]))
plt.axis('off')
plt.imshow(wc)
wc.to_file("/content/drive/MyDrive/Colab Notebooks/images/positive_tweets.png")

negative_tweets = data[data["sentiment"] == "Negative"]
print(negative_tweets.shape)
plt.figure(figsize=(20, 20))
wc = WordCloud(max_words=200, width=1600, height=800).generate(" ".join(negative_tweets["tweet"]))
plt.axis('off')
plt.imshow(wc)
wc.to_file("/content/drive/MyDrive/Colab Notebooks/images/negative_tweets.png")

plt.figure(figsize=(20, 20))
wc = WordCloud(max_words=200, width=1600, height=800).generate(" ".join(data["tweet"]))
plt.axis('off')
plt.imshow(wc)
wc.to_file("/content/drive/MyDrive/Colab Notebooks/images/tweets.png")
